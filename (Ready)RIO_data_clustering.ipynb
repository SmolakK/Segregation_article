{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67072590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import umap.umap_ as umap\n",
    "import seaborn as sns\n",
    "stat_path = r'RIO_statistics.csv'\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data\n",
    "stat_path_full = os.path.join(stat_path)\n",
    "df = pd.read_csv(stat_path_full,index_col=0)\n",
    "#Drop columns\n",
    "df = df.drop(['Num_of_loc','Num_of_days'],axis=1)\n",
    "#Transform RoG\n",
    "df['RoG k2'] = df['RoG k2']/df['RoG']\n",
    "df['RoG k4'] = df['RoG k4']/df['RoG']\n",
    "df['RoG k8'] = df['RoG k8']/df['RoG']\n",
    "#Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get potential combinations \n",
    "import itertools\n",
    "\n",
    "combinations = []\n",
    "for r in range(1, 3):  # For combinations of 1, 2, and 3 elements\n",
    "    combinations.extend(itertools.combinations(df.columns, r))\n",
    "#Recreate DF\n",
    "df_scaled = pd.DataFrame(df_scaled,columns=df.columns,index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d14e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "combs = {}\n",
    "for w in combinations:\n",
    "    scores_ch = {}\n",
    "    scores_db = {}\n",
    "    scores_s = {}\n",
    "    df_filter = df_scaled.drop([*w],axis=1)\n",
    "    \n",
    "    #Broken-stick criterion\n",
    "    pca = PCA(n_components=5,random_state=42)\n",
    "    principal_components = pca.fit_transform(df_filter)\n",
    "    thersholds = [1/(11-x) for x in range(1,6)]\n",
    "    stacked = np.vstack([pca.explained_variance_ratio_,thersholds])\n",
    "    n_components = (stacked[0,:]>stacked[1,:]).sum()\n",
    "    #Proper PCA\n",
    "    pca = PCA(n_components=n_components,random_state=42)\n",
    "    principal_components = pca.fit_transform(df_filter)\n",
    "    print(np.sum(pca.explained_variance_ratio_))\n",
    "    Z = linkage(df_scaled, method='ward')\n",
    "    # Elbow Method on Dendrogram\n",
    "    last = Z[-20:, 2]\n",
    "    last_rev = last[::-1]\n",
    "    idxs = np.arange(1, len(last) + 1)\n",
    "    plt.plot(idxs, last_rev)\n",
    "\n",
    "    acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "    acceleration_rev = acceleration[::-1]\n",
    "    plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "\n",
    "    k_elbow = acceleration_rev.argmax() + 2  # Add 2 to get the right number of clusters\n",
    "    print(f\"Optimal number of clusters based on the elbow method: {k_elbow}\")\n",
    "    \n",
    "    for n in range(2,6):\n",
    "        agglomerative_clustering = AgglomerativeClustering(n_clusters=n, metric='euclidean', linkage='ward')\n",
    "        clusters = agglomerative_clustering.fit_predict(df_filter)\n",
    "\n",
    "        # Add cluster labels to the original data\n",
    "        score = davies_bouldin_score(df_filter, clusters)\n",
    "        score2 = calinski_harabasz_score(df_filter,clusters)\n",
    "        score3 = silhouette_score(df_filter,clusters)\n",
    "        scores_ch[n] = score2\n",
    "        scores_db[n] = score\n",
    "        scores_s[n] = score3\n",
    "    print(\"Drop:\",w)\n",
    "    print(max(scores_ch,key=scores_ch.get),max(scores_ch.values()))\n",
    "    print(min(scores_db,key=scores_db.get),min(scores_db.values()))\n",
    "    print(max(scores_s,key=scores_s.get),max(scores_s.values()))\n",
    "    combs[w] = (max(scores_ch,key=scores_ch.get),max(scores_ch.values()),\n",
    "               min(scores_db,key=scores_db.get),min(scores_db.values()),\n",
    "               max(scores_s,key=scores_s.get),max(scores_s.values()),\n",
    "              (k_elbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame(combs).T\n",
    "cluster_df.columns = ['CCH','CH','CDB','DB','CS','S','EL']\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected parameters - process\n",
    "#Reading data\n",
    "stat_path_full = os.path.join(stat_path)\n",
    "df = pd.read_csv(stat_path_full,index_col=0)\n",
    "#Drop columns\n",
    "df = df.drop(['Num_of_loc','Num_of_days','Median from Home'],axis=1)\n",
    "#Transform RoG\n",
    "df['RoG k2'] = df['RoG k2']/df['RoG']\n",
    "df['RoG k4'] = df['RoG k4']/df['RoG']\n",
    "df['RoG k8'] = df['RoG k8']/df['RoG']\n",
    "#Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "#PCA\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f026d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dendrogram PLOT\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "cluster_colors = {\n",
    "    2: '#f0f922',    # Local Stayers\n",
    "    3: '#ee7850',   # Urban Wanderers\n",
    "    0: '#9c179d',  # Distant Commuters\n",
    "    1: '#0c0987', # Neighborhood Explorers\n",
    "}\n",
    "\n",
    "# Set a custom color cycle\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['grey']+['#9c179d','#0c0987','#ee7850','#f0f922'])\n",
    "\n",
    "# Compute the linkage matrix\n",
    "Z = linkage(principal_components, method='ward')\n",
    "\n",
    "custom_palette = ['#9c179d','#0c0987','#ee7850','#f0f922']\n",
    "cluster_labels = ['Distant Commuters','Neighborhood Explorers','Urban Wanderers','Local Stayers']\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z,truncate_mode='lastp')\n",
    "plt.xlabel('Samples',size=20)\n",
    "plt.ylabel(\"Distance\",size=20)\n",
    "# plt.show()\n",
    "\n",
    "# Add legend manually\n",
    "legend_patches = [\n",
    "    plt.Line2D([0], [0], color=color, lw=2, label=label)\n",
    "    for color, label in zip(custom_palette, cluster_labels)\n",
    "]\n",
    "\n",
    "plt.legend(handles=legend_patches, title=\"Clusters\", loc='upper right', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Agglomerative Clustering\n",
    "num_clusters = 4  # Adjust based on dendrogram analysis\n",
    "agglomerative_clustering = AgglomerativeClustering(n_clusters=num_clusters, metric='euclidean', linkage='ward')\n",
    "clusters = agglomerative_clustering.fit_predict(principal_components)\n",
    "\n",
    "# Add cluster labels to the original| data\n",
    "df['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ee724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebuild full data\n",
    "stat_path_full = os.path.join(stat_path)\n",
    "df = pd.read_csv(stat_path_full,index_col=0)\n",
    "df['RoG k2'] = df['RoG k2']/df['RoG']\n",
    "df['RoG k4'] = df['RoG k4']/df['RoG']\n",
    "df['RoG k8'] = df['RoG k8']/df['RoG']\n",
    "df['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65597cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average parameters\n",
    "df[[x for x in df.columns if 'Return' not in x and 'Median' not in x ]].groupby('Cluster').agg(['mean','std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Population\n",
    "df.groupby('Cluster').count().iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ea1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Population ratio\n",
    "df.groupby('Cluster').count().iloc[:,0]/df.groupby('Cluster').count().iloc[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9412b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA PLOT\n",
    "unique_clusters = np.unique(df['Cluster'])\n",
    "\n",
    "# Define custom colors for each cluster\n",
    "cluster_colors = {\n",
    "    2: '#f0f922',    # Local Stayers\n",
    "    3: '#ee7850',   # Urban Wanderers\n",
    "    0: '#9c179d',  # Distant Commuters\n",
    "    1: '#0c0987', # Neighborhood Explorers\n",
    "}\n",
    "names = ['Local Stayers','Urban Wanderers','Distant Commuters','Neighborhood Explorers']\n",
    "point_colors = df['Cluster'].map(cluster_colors)\n",
    "\n",
    "scatter = plt.scatter(principal_components[:,0],principal_components[:,1], c=point_colors)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel(\"PC 2\")\n",
    "\n",
    "# Create a legend\n",
    "# Add custom legend\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=f'Cluster {cluster}')\n",
    "    for color, cluster in zip(cluster_colors.values(),names)\n",
    "]\n",
    "plt.legend(handles=legend_handles, title=\"Clusters\", frameon=True, loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a70057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
